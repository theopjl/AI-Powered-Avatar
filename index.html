<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <title>AI Avatar Talk</title>
</head>
<script type="importmap">
    {
      "imports": {
        "three": "https://cdn.jsdelivr.net/npm/three@0.160/build/three.module.js"
      }
    }
  </script>
<style>
  body {
    margin: 0;
    overflow: hidden;
    background: black;
    font-family: 'Arial', sans-serif;
  }

  #input_prompt {
        position: fixed;
    bottom: 20px; /* espace par rapport au bas */
    left: 50%;    /* centre horizontal */
    transform: translateX(-50%); /* ajuste pour centrer parfaitement */
    width: 300px;
    padding: 12px 16px;
    border-radius: 25px; 
    border: none;
    background-color: rgba(200, 200, 200, 0.2); 
    color: white;
    font-size: 16px;
    outline: none;
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.2); 
    transition: background-color 0.3s, box-shadow 0.3s;
  }

  #input_prompt::placeholder {
    color: rgba(255, 255, 255, 0.6);
  }

  #input_prompt:focus {
    background-color: rgba(255, 255, 255, 0.15);
    /* effet léger au focus */
    box-shadow: 0 6px 10px rgba(0, 0, 0, 0.4);
  }
</style>

<body>
  <input id="input_prompt" type="text" placeholder="Type your prompt here" />
  <script type="module">

    import { io } from "https://cdn.socket.io/4.7.2/socket.io.esm.min.js";
    import * as THREE from "three";
    import { OrbitControls } from "https://cdn.jsdelivr.net/npm/three@0.160/examples/jsm/controls/OrbitControls.js";
    import { GLTFLoader } from "https://cdn.jsdelivr.net/npm/three@0.160/examples/jsm/loaders/GLTFLoader.js";

    const inputField = document.getElementById('input_prompt');

    const socket = io("http://localhost:5000");

    let halfBodyAvatar = './half.glb'

    let speech = './speech.mp3'

    let scene, camera, renderer, listener, sound, analyser;
    let avatar, mixer, idleAction;
    const clock = new THREE.Clock();

    let audioContext = new AudioContext();
    let audioBufferQueue = [];
    let isPlaying = false;

    init();
    animate();

    socket.on("audio_chunk", async (msg) => {
      // msg.data contient la chaîne base64
      const bytes = Uint8Array.from(atob(msg.data), c => c.charCodeAt(0));
      const buffer = await audioContext.decodeAudioData(bytes.buffer);
      audioBufferQueue.push(buffer);
      playQueue();
    });

    socket.on("audio_end", () => {
      console.log("Audio stream finished");
    });

    socket.on("conversation_text", (data) => {
      console.log("Full GPT response:", data);
    });

    function playQueue() {
      if (isPlaying || audioBufferQueue.length === 0) return;

      const buffer = audioBufferQueue.shift();
      const source = audioContext.createBufferSource();
      source.buffer = buffer;
      source.connect(audioContext.destination);

      isPlaying = true;
      source.start();

      source.onended = () => {
        isPlaying = false;
        playQueue();
      };
    }

    function init() {
      scene = new THREE.Scene();
      scene.background = new THREE.Color("black");

      camera = new THREE.PerspectiveCamera(45, window.innerWidth / window.innerHeight, 0.1, 100);
      camera.position.set(0, 0.6, 0.65);

      renderer = new THREE.WebGLRenderer({ antialias: true });
      renderer.setSize(window.innerWidth, window.innerHeight);
      document.body.appendChild(renderer.domElement);

      const light = new THREE.DirectionalLight(0xffffff, 2);
      light.position.set(1, 2, 3);
      scene.add(light);
      scene.add(new THREE.AmbientLight(0x404040));

      listener = new THREE.AudioListener();
      camera.add(listener);
      sound = new THREE.Audio(listener);

      const audioLoader = new THREE.AudioLoader();
      audioLoader.load(speech, (buffer) => {
        sound.setBuffer(buffer);
        sound.setLoop(false);
        sound.setVolume(1);
      });

      analyser = new THREE.AudioAnalyser(sound, 32);

      // Load Avatar
      const loader = new GLTFLoader();
      loader.load(halfBodyAvatar, (gltf) => {
        avatar = gltf.scene;
        avatar.position.set(0, 0.16, 0.1);
        avatar.scale.set(0.8, 0.8, 0.8);
        scene.add(avatar);

        // Idle Animation
        mixer = new THREE.AnimationMixer(avatar);
        const idleClip = gltf.animations.find(a => a.name.includes("idle_eyes_2"));
        if (idleClip) {
          idleAction = mixer.clipAction(idleClip);
          idleAction.play();
        }

        // Debug morphs
        avatar.traverse((child) => {
          if (child.morphTargetDictionary) {
            console.log("Morph targets:", Object.keys(child.morphTargetDictionary));
            console.log(gltf.animations);
          }
        });
      });

      inputField.addEventListener("keydown", async (event) => {
        if (event.key === "Enter") {
          const prompt = event.target.value;
          inputField.value = "";
          console.log("User prompt:", prompt);
          socket.emit("start_conversation", { prompt: prompt });
        }
      });

      window.addEventListener("resize", onWindowResize);
    }

    function onWindowResize() {
      camera.aspect = window.innerWidth / window.innerHeight;
      camera.updateProjectionMatrix();
      renderer.setSize(window.innerWidth, window.innerHeight);
    }
    
    function animate() {
      requestAnimationFrame(animate);
      const delta = clock.getDelta();
      
      if (mixer) mixer.update(delta);

      if (avatar && audioBufferQueue.length > 0) {

        const buffer = audioBufferQueue[0];
        let sum = 0;
        let channelData = buffer.getChannelData(0);

        for (let i = 0; i < channelData.length; i++) {
          sum += channelData[i] ** 2;
        }

        const rms = Math.sqrt(sum / channelData.length); // doing approx btw 0 → 1 

        avatar.traverse((child) => {

          if (child.morphTargetInfluences && child.morphTargetDictionary["mouthOpen"] !== undefined) {
            const i = child.morphTargetDictionary["mouthOpen"];
            child.morphTargetInfluences[i] = Math.min(rms * 10, 1); // doing amplification cause rms is probabily low
          }

          if (child.morphTargetInfluences && child.morphTargetDictionary["mouthSmile"] !== undefined) {
            const iSmile = child.morphTargetDictionary["mouthSmile"];
            child.morphTargetInfluences[iSmile] = Math.sin(Date.now() * 0.005) * 0.2 + 0.3;
          }

        });

      } else if (avatar) {

        avatar.traverse((child) => {

          if (child.morphTargetInfluences) {
            for (let i = 0; i < child.morphTargetInfluences.length; i++) {
              child.morphTargetInfluences[i] *= 0.9;
            }
          }

        });
      }

      renderer.render(scene, camera);
    }

  </script>
</body>

</html>